{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Churn Lab: TFDV + Wide & Deep (TensorFlow/Keras)\n\nThis notebook walks through a complete workflow:\n\n1. **Load & inspect** a tabular dataset (synthetic churn)\n2. **Data Validation with TFDV**\n   - statistics \u2192 schema \u2192 anomalies\n3. **Train a different Keras model**: **Wide & Deep** (Functional API)\n4. **Evaluate + export artifacts**\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nROOT = Path(\".\").resolve()\nDATA = ROOT / \"data\"\nART = ROOT / \"artifacts\"\nART.mkdir(exist_ok=True)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Load the dataset"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "df = pd.read_csv(DATA / \"train.csv\")\ndf.head()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "df.describe(include=\"all\").T.head(20)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) TFDV: statistics \u2192 schema \u2192 anomalies"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import tensorflow_data_validation as tfdv\nimport tensorflow as tf\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Helper: CSV -> TFRecord for TFDV\ndef _to_feature(v):\n    import pandas as pd\n    if v is None or (isinstance(v, float) and pd.isna(v)) or (isinstance(v, str) and v == \"\"):\n        return None\n    if isinstance(v, (int, bool)) or (isinstance(v, float) and float(v).is_integer()):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[int(v)]))\n    if isinstance(v, (float,)):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v)]))\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(v).encode(\"utf-8\")]))\n\ndef csv_to_tfrecord(csv_path: Path, out_path: Path):\n    import pandas as pd\n    df = pd.read_csv(csv_path)\n    with tf.io.TFRecordWriter(str(out_path)) as w:\n        for _, row in df.iterrows():\n            feat = {}\n            for col in df.columns:\n                f = _to_feature(row[col])\n                if f is not None:\n                    feat[col] = f\n            ex = tf.train.Example(features=tf.train.Features(feature=feat))\n            w.write(ex.SerializeToString())\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "tfdv_dir = ART / \"tfdv\"\ntfdv_dir.mkdir(exist_ok=True)\n\ntrain_tfr = tfdv_dir / \"train.tfrecord\"\ntest_tfr = tfdv_dir / \"test.tfrecord\"\nanom_tfr = tfdv_dir / \"test_anomalous.tfrecord\"\n\ncsv_to_tfrecord(DATA/\"train.csv\", train_tfr)\ncsv_to_tfrecord(DATA/\"test.csv\", test_tfr)\ncsv_to_tfrecord(DATA/\"test_anomalous.csv\", anom_tfr)\n\ntrain_stats = tfdv.generate_statistics_from_tfrecord(data_location=str(train_tfr))\ntest_stats  = tfdv.generate_statistics_from_tfrecord(data_location=str(test_tfr))\nanom_stats  = tfdv.generate_statistics_from_tfrecord(data_location=str(anom_tfr))\n\ntfdv.visualize_statistics(train_stats)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "schema = tfdv.infer_schema(train_stats)\ntfdv.display_schema(schema)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Validate clean test set\nanomalies_test = tfdv.validate_statistics(test_stats, schema)\ntfdv.display_anomalies(anomalies_test)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Validate intentionally corrupted dataset\nanomalies_bad = tfdv.validate_statistics(anom_stats, schema)\ntfdv.display_anomalies(anomalies_bad)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Train a Wide & Deep Keras model"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import tensorflow as tf\n\nCATEGORICAL_STR = [\"gender\", \"internet_service\", \"contract_type\", \"payment_method\"]\nCATEGORICAL_INT = [\"senior_citizen\", \"partner\", \"dependents\", \"paperless_billing\"]\nNUMERIC = [\"tenure_months\", \"monthly_charges\", \"total_charges\"]\nLABEL = \"churn\"\nID_COL = \"customer_id\"\n\ndef make_dataset(csv_path: Path, batch_size=64, shuffle=False):\n    ds = tf.data.experimental.make_csv_dataset(\n        file_pattern=str(csv_path),\n        batch_size=batch_size,\n        label_name=LABEL,\n        num_epochs=1,\n        header=True,\n        na_value=\"\",\n        shuffle=shuffle,\n        shuffle_buffer_size=4096,\n        ignore_errors=True,\n    )\n    ds = ds.map(lambda x, y: ({k: v for k, v in x.items() if k != ID_COL}, y))\n    return ds.prefetch(tf.data.AUTOTUNE)\n\ntrain_df = pd.read_csv(DATA/\"train.csv\")\ntrain_ds = make_dataset(DATA/\"train.csv\", shuffle=True)\nval_ds   = make_dataset(DATA/\"val.csv\", shuffle=False)\ntest_ds  = make_dataset(DATA/\"test.csv\", shuffle=False)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def build_preprocessing(train_df: pd.DataFrame):\n    inputs = {}\n    encoded = []\n    wide_features = []\n\n    # string categorical -> embeddings\n    for col in CATEGORICAL_STR:\n        inp = tf.keras.Input(shape=(1,), name=col, dtype=tf.string)\n        lookup = tf.keras.layers.StringLookup(output_mode=\"int\")\n        lookup.adapt(train_df[col].astype(str).values)\n        vocab = lookup.vocabulary_size()\n        x = lookup(inp)\n        x = tf.keras.layers.Embedding(vocab, output_dim=min(16, max(4, vocab//2)))(x)\n        x = tf.keras.layers.Reshape((-1,))(x)\n        inputs[col] = inp\n        encoded.append(x)\n\n    # int categorical -> one hot (wide)\n    for col in CATEGORICAL_INT:\n        inp = tf.keras.Input(shape=(1,), name=col, dtype=tf.int32)\n        lookup = tf.keras.layers.IntegerLookup(output_mode=\"one_hot\")\n        lookup.adapt(train_df[col].astype(int).values)\n        x = lookup(inp)\n        inputs[col] = inp\n        wide_features.append(x)\n\n    # numeric -> normalization\n    for col in NUMERIC:\n        inp = tf.keras.Input(shape=(1,), name=col, dtype=tf.float32)\n        norm = tf.keras.layers.Normalization()\n        norm.adapt(train_df[col].astype(float).values.reshape(-1,1))\n        x = norm(inp)\n        inputs[col] = inp\n        encoded.append(x)\n\n    wide = tf.keras.layers.Concatenate()(wide_features)\n    deep = tf.keras.layers.Concatenate()(encoded)\n    return inputs, wide, deep\n\ninputs, wide, deep = build_preprocessing(train_df)\n\nx = tf.keras.layers.Dense(64, activation=\"relu\")(deep)\nx = tf.keras.layers.Dropout(0.25)(x)\nx = tf.keras.layers.Dense(32, activation=\"relu\")(x)\ncombined = tf.keras.layers.Concatenate()([wide, x])\nout = tf.keras.layers.Dense(1, activation=\"sigmoid\")(combined)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=out)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[tf.keras.metrics.AUC(name=\"auc\"), tf.keras.metrics.Precision(name=\"precision\"), tf.keras.metrics.Recall(name=\"recall\")]\n)\nmodel.summary()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "history = model.fit(train_ds, validation_data=val_ds, epochs=5)\neval_metrics = model.evaluate(test_ds, return_dict=True)\neval_metrics\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Export artifacts"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "model_dir = ART / \"model\"\nmodel_dir.mkdir(exist_ok=True)\nmodel.save(model_dir / \"saved_model\", include_optimizer=False)\nprint(\"Saved to\", model_dir/\"saved_model\")\n",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}